<!DOCTYPE html>
<html>
<head>
    <title>Get more from existing resources. Introduction to distributed systems.</title>

    <!-- meta -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- css -->
    <link rel="stylesheet" href="../../../css/bootstrap.min.css">
    <link rel="stylesheet" href="../../../css/ionicons.min.css">
    <link rel="stylesheet" href="../../../css/pace.css">
    <link rel="stylesheet" href="../../../css/custom.css">
    <link rel="stylesheet" href="../../../css/lc_gif_player.css">

    <!-- js -->
    <script src="../../../js/jquery-2.1.3.min.js"></script>
    <script src="../../../js/bootstrap.min.js"></script>
    <script src="../../../js/pace.min.js"></script>
    <script src="../../../js/modernizr.custom.js"></script>
</head>

<body id="single">
<div class="container">
    <div class="container">
        <header id="site-header">
    <div class="row">
        <div class="col-md-4 col-sm-5 col-xs-8">
            <div class="logo">
                <h1><a href="../../../index.html"><b>Konoplev's</b> Blog</a></h1>
            </div>
        </div><!-- col-md-4 -->
        <div class="col-md-8 col-sm-7 col-xs-4">
            <nav class="main-nav" role="navigation">
                <div class="navbar-header">
                    <button type="button" id="trigger-overlay" class="navbar-toggle">
                        <span class="ion-navicon"></span>
                    </button>
                </div>

                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">
                        <li class="cl-effect-11"><a href="../../../index.html" data-hover="About">About</a></li>
                        <li class="cl-effect-11"><a href="../../../en/thoughts/index/1.html" data-hover="Thoughts">Thoughts</a></li>
                        <li class="cl-effect-11"><a href="../../../en/programming/index/1.html" data-hover="Programming">Programming</a></li>
                    </ul>
                </div><!-- /.navbar-collapse -->
            </nav>
            <div id="header-search-box">
                <a id="search-menu" href="#"><span id="language-icon" class="ion-android-globe"></span></a>
                <div id="language-form" class="language-form">
                    <div id="searchform">
                                                
                        <div class="text-center"><a href="../../../ru/programming/index/1.html" data-hover="About">По-русски</a><div>
                            <button type="submit"><span class="ion-android-globe"></span></button>
                        </div>
                        </div>
                    </div>
                </div><!-- col-md-8 -->
            </div>
</header>    </div>
</div>

<div class="content-body">
    <div class="container">
        <div class="row">
            <main class="col-md-12">
                <article class="post post-1">
                    <header class="entry-header">
                        <h1 class="entry-title">Get more from existing resources. Introduction to distributed systems.</h1>
                        <div class="entry-meta">
                            <span class="post-date"><a href="#"><time class="entry-date"
                                                                      datetime="2017-12-31T23:00:00Z">Jan 1, 2018</time></a></span>

                            <span class="post-category">
                                                                    <a href="../tag/software_architecture/1.html"> #software_architecture</a>
                                                                    <a href="../tag/microservices/1.html"> #microservices</a>
                                                            </span>

                                                </div>
                        <figure class="img-responsive-center">
                            <img class="img-responsive" src="./get_more_from_existing_resources._introduction_to_distributed_systems..png"
                                 alt="Developer Image"/>
                        </figure>
                    </header>
                    <div class="entry-content clearfix">
                                            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Rewrite!</p>
</div>
<div class="paragraph">
<p>Everyone who tried to fix some race conditions in multithreading application knows that this is much much eseir to run programm sequentially. Everyone who tried to understand some unexpected behaviour of big distributed system knows that this is much esear to debug monolith application. Sometimes microservices and complex systems are just a cargo cult. Big companies do that, they speak about it on conferences, that&#8217;s why we also have to do that.
Here I&#8217;m going to talk about how did we come to a distributed world. And this is a good start of talking about disrtibuted systems. I&#8217;m going to describe some basic things. Maybe you&#8217;re familiar with them but I need a fundament to start talking about complex stuff.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_idle_cpu">Idle CPU</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Many years ago computers were big and expensive (<a href="https://en.wikipedia.org/wiki/IBM_7090">IBM 7094 costed $18 million</a> for example). If some program&#8217;s waiting for a data from input device to start processing it, the very expensive CPU simply sit idle. To utilise CPU better new Operating Systems and hardware allowing to have multiple programs in memory were developed. So while one program waits on IO, another one having enough data could use CPU. This is how CPU could be utilised almost 100% of the time.</p>
</div>
<div class="sect2">
<h3 id="_concurrent_processing">Concurrent processing</h3>
<div class="paragraph">
<p>What if there are multiple programs ready to use CPU? Which one should be ran first? OSs have a <a href="https://en.m.wikipedia.org/wiki/Preemption_(computing)">preemtive scheduler</a> giving each program a fair share of CPU and keep all parts of the system busy. So if we have tow programs PR1 and PR2, the scheduler could allow PR1 to use CPU during some time, then put it on hold and let PR2 to use CPU, then put PR2 on hold, switch to PR1 one and so on. If some program is waiting on IO, scheduler will not give CPU this program until the IO is done. So, despite the fact that there is only one CPU we have programs running simultaneously, or concurrently.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="1.PNG" alt="1">
</div>
</div>
<div class="paragraph">
<p>
<img src="https://lh3.googleusercontent.com/roRLyYjpLnaQJ8b3-HI7fCe0HKujhwXM-L0MIUorBAlEHyFjzsUypjwQUWnEYcNn5DjdB9r2Wo09JQMpiAss_D-TzWrCRqolrpgNJXHL6Avk-ZCFtW3twfa3ndE5bawHoBTKDyylzF6UTwJLFN7islgt41sWv1wlP5mjnVYUTMkKSJEy2Yg9H9MwaleRCjvLrXfC4LA5xlolSRmsap5dMXaWXY3I0rhjzTFnv7-KiGWT7LgaREHwx3Ullu8oO7UOWOHdCAdx5eNUuKG1yStvlxdL_gafdTfOtg8FB-SJ_b_qDoVLFM_7ZbI1Onh5X1PyB8q0_WN1t5qX_4SVEQw61dUf13O9rATs8VPWW9hY7huewTdAZNe9u3woKF2jEb9OGUurQNhaFChcqS6EyX1J-FVEBJvPaz6lyq9cvwvPS3h3psvTP3xuIIYNNnTcwJ4RfM0tuYYSUgvPii1f74KYtuQ-FKgNcd_HN3plBVx8srDuEmB3qWOgD0kEwtTs_tDWbL6q4KganTi9rNT9P-J4ldRhitjBWVAMwiLd2G7coFLn69g6VGGDq5wV1ePDN6vvtqOpu8V2RrE5849BOuSrBUmmAFTUvsifZCnGN4UQHIgbQLd3lkKGyEfvhJ7Mc7Thre5bfMbU3qFG16RZDtDew5DfMJG1A1YzgfiS99BWcBAJiCmHoClCaJB5KoOqpgssLC2MvY_h4pTgmoGue066CyLA=w930-h1344-no" class="img-responsive" alt="picture is removed"></img>
</p>
</div>
<div class="paragraph">
<p>As you can see the time is very important, that&#8217;s probably why a program with its state (allowing to know which instruction should be run next when scheduler unpause the program and what resources the program needs) was called a process (something which takes some time to be done). A process have one or many execution threads. A thread needs less resources than process, so on modern OS&#8217;s it&#8217;s cheaper to create a thread than a process.</p>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_processing">Parallel processing</h3>
<div class="paragraph">
<p>Of cause if there is only one CPU you can&#8217;t get a real parallelisation. If there are 100 processes calculating something (no IO) you&#8217;ll get 100 results of the programs 100 times slower than having these programs running on their own CPU independently. In reality it&#8217;s even slower, because switching between programs also needs some time.</p>
</div>
<div class="paragraph">
<p>Real parallelisation is possible if your CPU has multiple cores. So different threads could be run on different CPU cores and be done in parallel.</p>
</div>
<div class="paragraph">
<p>As I mentioned before the time is very important. When developers got a possibility to run multiple threads in parallel, they started to think about time optimisation. Nobody likes to wait. A long task could be split into small parts to be done in parallel and a result could be available quicker. For example, we&#8217;re going to count how often each word is used in a 1000 pages book. We could have one thread reading the whole book and counting words. But to get result quicker, we can split 1000 pages into 100 pages chunks, run 10 threads counting words running on its own CPU core, pass each chunk to its own thread and get result 10 times faster.</p>
</div>
<div class="paragraph">
<p>
<img src="https://lh3.googleusercontent.com/Ook_kloTF6L-9ZdwL_-4tjytIPP5ivryiuFCpSE_9WzwNo4yVQMdRpz8BjnvPmmIsQfRQrups6w5egmusYEoQGLPBEh7O530XJV2sy7N874gUJ_8_omsDfmyHsmkFvzxMPzUqLC5wvklB3pFpNaeE-lPTNLAqJkxNAVD8VQ9QcmVR3ExMWfquDjmiH2l5CN2CIS7hzUq8Cv8RUiNYS_dHRJCOynCvDRcEBUz-s6Kakn8bZUR7Mx4C4RNXxgKbF8mJtworJ16Fvf4Cosky7HiDHC9So8yaKd236fsBWk7FyD8_5NcKRqQxGJaUPaJe6gaTfeLhWAoC5xjQG79TuT32Lg7C0MKzLPeRUsLl7gLxlnjsIe_s0h8LWOsWOVt1mBWl4F8dmonfIwilpFdljDq21dQuQ4QFEv0Oa9PSP8Xr0F6Qjc8csmAKlIrpg84uBorj7dKW8jrpEpc_Y-Q7Q6dWzVD5lhDFPW_eYJ_QFRETlS-LBo_fcnrXUZFXxyu0cOllmzsd5Blqe7qYfpcqrVuttADOO1TpprvBJGyPN500gehF3LvdIFnThBJsbxX4CPlnJMxqvot8vbrbpn97aO0RvqzKIeYBK0elJ4zP07PyGv_AqV5bZc1YA6ONEVzEsZRp5H739OCSSGixUID0IxMAw8fn3ErkehcPeDlrLJLZL_J7bqMNAJgsjNrS3bRrbwYnE7Tt7pSWTaGrzBZIqKt1ay3=w1990-h1344-no" class="img-responsive" alt="picture is removed"></img>
</p>
</div>
<div class="paragraph">
<p>Of-cause, 10 times boost is not free. We have to do extra work to implement task splitting and merging.
Our tasks are done faster, but we got new challenges. Threads should communicate with each other. There should be a thread waiting for all 10 tasks is completed and after that all results are combined. So speedup of the whole job can&#8217;t be faster than speed of smallest sequential task (you can read about <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amadahl&#8217;s law</a> to find more details). The picture simplifies things. In real life, probably different batches will take different time to be processed. So, to improve performance threads processing pages could communicate to help each other. Imaging for example that one of the 100 pages chunk has much more words than other chunks. So, we got 9 chunks ready but one of the threads still working on its chunk. We have to wait for this thread to get the final result. Instead, idle threads could still work from busy thread to get job done faster.</p>
</div>
<div class="paragraph">
<p>
<img src="https://lh3.googleusercontent.com/97WNjAdi3xyObxSlWyuh0gj5O2Tw4O29peWiAh4oEicuTgbPuIGYPjluSH0DdHC6dn2oZTP-zYRA7f8SbhpN0HDe9J7xBR_aABcVbF6ELadL_QfNfo2FJRaKB6puoXV8HkMkhPlAIjP4fvaIxvIG95-sq8aiRO90woEkj4XLl8dfvFIXj2O9Uah7p0erMRQWUkEGmOG0GNP2KR809NU8Jo3Q_U_H4U_HTfvBHRqt27FCczU6Mz-5wvh1TXxFH5XsgmKCdXfo6YtNydcDVAGke2peoueReWRYa7khSHuc_5BWKSIGQE_4i7oN660KM-ziu79kmDJfLBghFX7Www1OhPEshhMnxZU5icRuslJslRs5z7Xk-RQJd5eFbtykeVMKVzreDbThQuY208-Xqoi_qKWxJ9hZAlD-61EE14Bh5aeac47Xm2de8lRWO5s-UFKshqVgKVZfL3QjzJRx-l3xgavQxcMjE7shmFKCKjs-C2NudOgn9YL4WZaJRJt51N2O_5hnsZye2Nq5z2T0-Q0n1AzNk11a9lqlpJq2T6oDtFVSDFyBfg8ej-AbGDCC6pygoEol4vkm_9Bysf0zDfJETsm1dRrCI6L7xF-nE0Eh9LuznWaAbM5nHSTmIzHk-1kpQOUbGSlS4I5RDcQlmtS3JgiiIPhLv9rr5Kdg-m_H5KtPIWIeEf1WdzwigC18IER08oZsbLNTmMk_zMRMJ280DQdo=w2185-h1307-no" class="img-responsive" alt="picture is removed"></img>
</p>
</div>
<div class="paragraph">
<p>But to allow that we should be able to split chankes better and implement job stealing (or find some library providing this functionality). Any improvements need extra work.</p>
</div>
</div>
<div class="sect2">
<h3 id="_synchronous_and_asynchronous">Synchronous and asynchronous</h3>
<div class="paragraph">
<p>So instead of counting words synchronously in one thread, we count them asynchronously and get result faster. The asynchronous means that we loose a control on order of execution. Some tasks are done, some tasks are still in progress and we never know the order of completion. It complicates things but while word counting threads are running we can do something else in a main thread. Load a next book from disk for example. A merge task could be run asynchronously also.</p>
</div>
<div class="paragraph">
<p>So, generally speaking a task could be run asynchronously if we can do a job without task results. As soon as result is ready we&#8217;ll use it, but for now we can do something else.</p>
</div>
<div class="paragraph">
<p>There are different ways to get and process results of asynchronous task, but we&#8217;ll talk about them in the next post, because it&#8217;s a big topic.</p>
</div>
</div>
<div class="sect2">
<h3 id="_blocking_io">Blocking IO</h3>
<div class="paragraph">
<p>Now let&#8217;s back to IO and discuss improvements we can make here. Say that we&#8217;re developing a server processing incoming HTTP requests. To process each request a connection between server and client should be established. Our server should keep a connection to each client sending a request until the response is sent back. So the basic solution would be to create a thread for each connection and process each client in its own thread. We have a thread listening on server socket, as soon as connection is established, we create a socket for that and start a new thread, the thread communicates with client. Each thread has it&#8217;s own connection, and blocks on wait. So, until data is received it does nothing.</p>
</div>
</div>
<div class="sect2">
<h3 id="_nonblocking_io">Nonblocking IO</h3>
<div class="paragraph">
<p>Too many threads. One thread per client is expensive. How to reduce number of threads? Unblocking IO. It returns -1 if there is no data.</p>
</div>
<div class="paragraph">
<p>Another way is to use unblocking IO. It&#8217;s called busy waiting. A thread sleeps most of the time, periodically check for a data from client and if there is no data sleeps again. It means that thread is busy, it burns CPU cycles and occupy some memory, but do nothing useful because it&#8217;s waiting. The problem is that a thread never knows when client is ready to send data. If we check for data too often then we use too much CPU. Maybe client just opened connection and went for a lunch. If we check too rarely we response slow. A client sent a request, but we still sleep and we&#8217;ll check the request only after some time. And you can&#8217;t find a good timeout for a sleep because you can&#8217;t predict future. Also creating a thread per connection is expensive, each thread needs a memory to keep its state and high number of threads make scheduler to work hard.</p>
</div>
<div class="paragraph">
<p>How many requests is it possible to process if you have only one thread? A lot! It&#8217;s easy to process thouthands of parallel requests. How? As we saw, most of the time our request processor waits for data in opened connections. Modern OSs allows to read and write data in nonblocking mode. Nonblocking means that read or write returns immediately. Say, we try to read some data from some resource in non blocking mode. If the resource has some data it&#8217;s returned immediately, but if there is no data an error returned. So we don&#8217;t spend time waiting on IO, do some another useful stuff and try to read later. Obviously there is a gap between the data is received from some resource by kernel and our next read attempt. The data should be kept somewhere. So there is a buffer in a memory managed by kernel. Access to the buffer is much <a href="https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html">faster than to any IO resource</a>. So we assume that the data is read from the buffer immediately.</p>
</div>
</div>
<div class="sect2">
<h3 id="_multiplexing_and_event_loop">Multiplexing and Event loop</h3>
<div class="paragraph">
<p>There are <a href="https://en.wikipedia.org/wiki/Epoll">epoll</a> (Linux) and <a href="https://en.wikipedia.org/wiki/Kqueue">kqueue</a> (FreeBSD) system calls allowing to monitor opened connections and subscribe to messages. It&#8217;s called <a href="https://people.eecs.berkeley.edu/~sangjin/2012/12/21/epoll-vs-kqueue.html">IO multiplexing</a>. The idea is that your server creates file descriptors (actually it&#8217;s called sockets but <a href="https://en.wikipedia.org/wiki/Everything_is_a_file">everything is a file</a> in UNIX) for each incoming connection and add them to a function which provides your program with any updates occurred in file descriptors. So instead of waiting (pulling data) from sockets we subscribe for a data to be notified when it&#8217;s ready (data is pushed).</p>
</div>
<div class="paragraph">
<p>Our server could use IO multiplexing to process all incoming requests in one thread.
So the algorithm is following:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Block and wait until batch of sockets are ready to be read.</p>
</li>
<li>
<p>Read all messages from the sockets and process them one by one in a single thread. While you processing them other requests could come, but you ignore them for now. Received requests are stored in kernel buffers. So when we read them on step 1, we don&#8217;t wait for any data and read from fast RAM.</p>
</li>
<li>
<p>Add responses to sockets ready to be writable. The responses are added to buffers, so we don&#8217;t wait for IO here.</p>
</li>
<li>
<p>Go back to step 1.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The described algorithm is called <a href="https://en.wikipedia.org/wiki/Reactor_pattern">reactor pattern</a>.</p>
</div>
<div class="paragraph">
<p>This infinite loop allows to process a lot of requests, just because you are not waiting on IO, you spend this time for request processing.</p>
</div>
<div class="paragraph">
<p>It works perfect. This approach is used in nginx, tornado, node.js, python asyncio, vert.x, netty.</p>
</div>
<div class="paragraph">
<p>To organise your code, you can create specific handlers for specific message types. So our event loop checks message type, determines which handler can process the message and pass it to the handler. Let&#8217;s say that one of the handlers processes messages encoded by some expensive algorithm, <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA</a> for example. To decode RSA you need a lot of CPU time, so this handler will work long time and block entire loop. If you have many RSA encoded messages the loop is blocked and all other messages are waiting and not processed also. This is the downside of event loop. A developer should be very careful to not block an event loop.</p>
</div>
<div class="paragraph">
<p>As you see the handlers can&#8217;t block the loop, can&#8217;t wait on IO or do any time-expensive calculations. So if you really need to do something expensive the only way is to run the handler asynchronously.</p>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_processing_workers">Parallel processing. Workers</h3>
<div class="paragraph">
<p>A thread or process is usually called worker. As it was mentioned before it&#8217;s expensive to create a thread (or process) per each call. To overcome this problem a server could create a pool of processes (workers) and reuse them.</p>
</div>
<div class="paragraph">
<p>It&#8217;s good because if some request needs a lot of time to be processed or need to wait on IO and others could be processed quickly, quick requests are not blocked, because long request occupied separate thread or process and run in parallel.</p>
</div>
<div class="paragraph">
<p>So it&#8217;s easier to create a handler for workers than for event loop in terms of programming. Your handler lives in isolated environment, it doesn&#8217;t share resources with other handlers (at least theoretically) and each request could be processed independently.</p>
</div>
<div class="paragraph">
<p>Should worker be a thread or process? It&#8217;s a trade-off between safety and speed. It&#8217;s faster to create a thread, it has less overhead. It&#8217;s faster to communicate between threads. But if a thread failed the entire server is failed because there is only one process.</p>
</div>
<div class="paragraph">
<p>What should we do if we have more requests than the size of the pool? We can increase a pool size (and spend some time for threads creating) or just wait until there are some free threads in the existing pool. To don&#8217;t block a processor reading incoming requests a queue could be added in front of the thread pool.</p>
</div>
<div class="paragraph">
<p>Of-cause a queue is a solution allowing to survive short spikes in number of incoming requests. If there are more requests than the queue size, then you will get the same problem again. You should decide what to do with the requests you are not able to process. Even if you use unbounded queue the number of requests you can put to the queue is limited, because you the memory size allocated for your process is always limited.</p>
</div>
<div class="paragraph">
<p>Usually pool size and queue size are configurable. To find a good value for a pool size it&#8217;s required to know how many requests per second the application is going to receive and how long each request is processed in average. To find a good value for a queue size you should know the maximum amount of requests per second and maximum processing time.</p>
</div>
<div class="paragraph">
<p>So workers model is very popular also. It&#8217;s used in apache server, wsgi, fastcgi (i.e. PHP, Rails, Django, Flask) and Tomcat.</p>
</div>
<div class="paragraph">
<p>Of cause optimisation of CPU usage is not free. We get new problems because the system became more complicated. The threads using CPU should not interfere with each other. So if some thread is failed or stuck others should continue working. Partially this problem is solved by the scheduler trying to give each program a fair share of CPU. But threads could use same memory and trying to write to same file and potentially corrupt the things. Or one thread could wait for results providing by another thread and never get them because another thread is stuck. And these problems should be solved by developer. We&#8217;ll dive into these problems a little bit later.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_idle_cluster">Idle Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Many backend products follow very similar lifecycle. At the beginning it&#8217;s something simple, maybe just proof of concept. It receives some requests from frontend, fetches some data from a database and sends responses back. It&#8217;s running on a single server. It&#8217;s easy to deploy, monitor and maintain.</p>
</div>
<div class="paragraph">
<p>But business is growing. New features are implemented. We need to integrate some billing system, for example. Also, it&#8217;s not enough to just process some requests now. Business needs to know some statistics. How many requests did we receive, what data is requested more often, and so on. Our simple service became not so simple.</p>
</div>
<div class="paragraph">
<p>New users are coming, we have to serve more and more requests, we have more data and finally, we grow over our old good server. We don&#8217;t have enough memory or CPU, our service is slow and fails sometimes. Obviously, we need to scale somehow.</p>
</div>
<div class="paragraph">
<p>The easiest way is just to buy a more powerful server with more CPU and memory. It&#8217;s called vertical scaling and it could be a solution. But if your business continues growing (which is good) the problem will occur again (which is bad).</p>
</div>
<div class="paragraph">
<p>Another solution is to run our backend on different hosts. We don&#8217;t even need to change our product significantly. Just add a load balancer in front of our backend to serve requests evenly between the hosts. It&#8217;s called horizontal scaling. And we need to run our database on a very big host because it will receive more requests (and it&#8217;s vertical scaling for our database).</p>
</div>
<div class="sect2">
<h3 id="_replication">Replication</h3>
<div class="paragraph">
<p>If you have been using your computer for a long time you probably had it failed at least once. Usually, it&#8217;s a software problem which can be solved by a reboot. Rarely it&#8217;s a hardware problem and repairing requires a hardware change.</p>
</div>
<div class="paragraph">
<p>To avoid a downtime because of such failure important services are usually run on multiple hosts. It&#8217;s called replication. There are different types of replication and we discuss them later. But the general idea is to always have a backup replica of a service to switch to if the main service is down. And it&#8217;s good to have a replica for our database also.</p>
</div>
<div class="paragraph">
<p>So now, if one of the servers is down we can survive because load balancer redirects all requests to the alive host.</p>
</div>
<div class="paragraph">
<p>The solution looks very similar, isn&#8217;t it? I&#8217;ve mentioned that in terms of failover it&#8217;s better to have two processes instead of two threads running within one process. And now I&#8217;d say that it&#8217;s even better to have these processes running on different hosts because our program could be failed not because of our bug, but because of OS failure or hardware failure.</p>
</div>
<div class="paragraph">
<p>I also mentioned that threads and processes should not interfere with each other. In practice, it&#8217;s really difficult to achieve.</p>
</div>
<div class="paragraph">
<p>So in our example, a user could try to create a record in the database (about adding a new item to the basket, for instance) and immediately perform an action which needs this record (complete a purchase). Load balancer could send a request about the record creation to instance A and request to perform an action could be sent to instance B. If instance A is slower than instance B then B performs the action without a knowledge about the record, and purchase will not contain the item which had to be added by instance A.</p>
</div>
<div class="paragraph">
<p>As you can see replication increases a failover of our system, but brings new problems.
There are a lot of potential problems, but this is the topic for the next posts. Now let&#8217;s think about resource utilization.</p>
</div>
</div>
<div class="sect2">
<h3 id="_microservices">Microservices</h3>
<div class="paragraph">
<p>We didn&#8217;t change our backend and it still does a lot of things. For example, it serves requests and it provides some statistics about them. Probably our statics will show that the number of users correlates somehow with the time of the day and day of the week. We have fewer users at 4 am than at 8 pm, for example. So we should reserve some resources for peaks.</p>
</div>
<div class="paragraph">
<p>And most of the time these resources are not used. We have multiple hosts. So idle resources are multiplied. While most of the users sleep at 4pm and we don&#8217;t need to spend resources to serve user request the reserved resources could be used by another task. While requests processing task is waiting for input, host resources could be utilized by statistics calculating task which has enough data to process. It sounds familiar again, doesn&#8217;t it? I started this post by discussing idleness of expensive CPU. And now, we have an idle cluster.</p>
</div>
<div class="paragraph">
<p>But this task is a part of our big service. So, while we calculating statistics some memory contains part of our service which processes requests. But it&#8217;s not needed for statistics processing and this is wasting of memory. So, it&#8217;s probably a good idea to split our service. There are more reasons to do that.</p>
</div>
<div class="paragraph">
<p>Some parts of our service require more memory than CPU, other parts need more CPU than memory. Having these parts separated allows us to have smaller dedicated hosts which are usually cheaper than a big host meeting all requirements. And even if we have a cluster of huge powerful hosts splitting of big services allows us to slice the hosts and consume existing resources better. Take a look at the picture.</p>
</div>
<div class="paragraph">
<p>So you have the same number of hosts but you got all you services replicated.</p>
</div>
<div class="paragraph">
<p>It&#8217;s like putting sugar to a glass. If you put sugar cubes in a glass, less sugar will enter the glass than if you split the cubes and poured the sugar.</p>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_processing_2">Parallel processing</h3>
<div class="paragraph">
<p>One more intent for vertical scaling is time. Maybe you have a huge amount of statistical data and even if you have very powerful machine processing of this data will take forever. So it&#8217;s a good idea to split a huge set of data into small pieces and process them in parallel on multiple machines. Remember our example of the number of words calculating?</p>
</div>
<div class="paragraph">
<p>Looks very similar, right? But now the rectangles are hosts, not processes.</p>
</div>
</div>
<div class="sect2">
<h3 id="_distributed_world_challenges">Distributed World Challenges</h3>
<div class="paragraph">
<p>This is how we started to split monoliths into micro-services. As usual, this gave rise to new problems. Statistics results should be available to a service which processes UI. And info about requests and responses should be available for statistics service. So, our services should communicate with each other. The idea of horizontal scaling is not new. RPC was developed in 80-s. After that CORBA, SOA, and other protocols were developed. Also, to communicate services should know about each other. So we should implement service discovery somehow. It&#8217;s more difficult to deploy and maintain our software. You can deploy to a single service even manually, it&#8217;s not so if you have dozens or hundreds of servers. It&#8217;s better to do it automatically to avoid errors. But all these problems are more or less solved and we will talk about standart solutions later.</p>
</div>
<div class="paragraph">
<p>The main problem is that you are in a distributed world now. Services live there own lives. Some of them could fail, some of them could be slow. You have exactly the same problems as you have with the threads running on a host. And solutions are more or less similar. For example, you can split your backend into API and workers. You have multiple instances of API and workers. It looks very similar to a thread pool. There are two ways to process increased traffic: autoscaling (add new workers to pool) or just add messages to a queue and hope that the traffic will low and you already have resources to process requests. Remember the event loop and workers? Same problems, same solutions.</p>
</div>
<div class="paragraph">
<p>So, most of the problems are similar. That&#8217;s why the next post will be about multithreading. If you know problems and solutions in the concurrent world, you could identify and solve the same problems in a world of distributed services.</p>
</div>
<div class="paragraph">
<p>But, of-cause, distributed world brings new challenges. Communication through the network is slower and less predictable then communication between threads within the same host. If service A sent a message to service B and didn&#8217;t receive a response it could mean absolutely different problems. Maybe service B is down, maybe it received the message but failed after that, maybe the service B is alive and processed the message, but network become unavailable and response has not been delivered. So service A have no idea should it resend the message or not. If service B doesnt respond because it&#8217;s overloaded by incomming messages. Resending undelivered messages increases back pressure and make service B life even harder.</p>
</div>
<div class="paragraph">
<p>The network could become a bottleneck. For example, if you split the huge amount of statistical data to process it in parallel on a fleet of nodes how the parts of data are delivered to the nodes? The only way is the network. So, sometimes it&#8217;s better to bring a processor to data instead of sending data to the processor through the slow network. This is what map-reduce systems usually do.</p>
</div>
<div class="paragraph">
<p>As you see we have a lot of interesting challenges. So, welcome to the distributed world. It will be fun!</p>
</div>
<div class="paragraph">
<p>So, for example if you have a relational database running on some server. On some rainy day the server&#8217;s disk could be dead. Even if you have a database&#8217;s back-up it takes many hours to fiscally change the disk and restore the back-up. To avoid database downtime you could run one more instance of the database on a separate host, periodically copy a data from main database and in case of failure just switch to the replica database. Of-cause services writing to this database are replicated also. So, modern backend is usually running on multiple hosts within a cluster.</p>
</div>
<div class="paragraph">
<p>Data is growing exponentially and sooner or later our database server could become</p>
</div>
<div class="paragraph">
<p>If you have a service which is required to be available 24/7 it&#8217;s a good idea to have it running on multiple computers, so if one of them failed other hosts could continue serving requests. This is how we got clusters of computers where multiple instances of services are run.</p>
</div>
<div class="paragraph">
<p>Now, imagine that there are 2 services (A and B) and 2 exactly the same hosts (H1 and H2). What configuration do you need to have the services reliable? You can run service A on H1 and H2 and buy 2 more hosts to run service B on them. Fast and straightforward solution. But before making any decisions it&#8217;s good to measure how much resources the services need. Maybe you&#8217;ll find out that service A never consumes more than 30% of resources and service B also needs maximum 30% of the existing host. So you already have enough hosts. Probably it&#8217;s waisting of money to buy more servers in this case.</p>
</div>
<div class="paragraph">
<p>picture
H1 H2</p>
</div>
<div class="listingblock">
<div class="content">
<pre>A A</pre>
</div>
</div>
<div class="paragraph">
<p>B B</p>
</div>
<div class="paragraph">
<p>Same as CPU. We have expensive host, but a service running on this host doesn&#8217;t utilise 100% of resources. To utilise the host better it&#8217;s a good idea to run more services on this host. So the host is not idle.</p>
</div>
<div class="paragraph">
<p>Это как пытаться заполнить стакан кубиками с сахаром. Как много сахара влезет в стакан, если сахар в кубиках? Не много. Между кубиками есть незаполненное простраство. А тебе разобьем кубики на мелкие части. Превратим их в сахарный песок. Влезет гораздо больше сахара, потому что мы заполняем сахором пустоты.</p>
</div>
<div class="paragraph">
<p>Если на вашем сервере программа использующая 60% ресурсов, то вторая такая программа в сервер уже не влезет. Но если разбить большую программу на маленькие, то на сервер влезет гораздо больше. Это путь, которым человечество пришло к микросервисам.</p>
</div>
<div class="paragraph">
<p>And we have the same problem. Several services running on the same host should be independent and shouldn&#8217;t interfere each other. This problem could be solved by virtualisation or containerisation. The idea is to have multiple operation systems running on the same host. Each OS thinks that it runs on real hardware but in reality the hardware is emulated by hypervisor. We&#8217;ll take a closer look at this approach in the next posts, but for now I&#8217;d say that it solves the problem because we can have different services implemented for different OSs running on the same host. And even if the service breaks something and it leads to OS crush another service and its OS is not impacted and running.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_price_of_optimisations">The price of optimisations</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_parallelization_different_ways_same_problems">Parallelization. different ways same problems.</h3>
<div class="paragraph">
<p>To summarize.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If some resource is idle, to utilize it better we run more resource consumers using the resource while others are busy using other resources. In first example the resource is CPU, resource consumer are threads. In the second example the resource is entire host and resource consumer is some service running on the host.</p>
</li>
<li>
<p>Better utilization is not free. We got some new problems to solve. Resource consumers should be isolated or at least do not affect each other.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Of cause, in real live threads or services are not isolated. To complete there job they need results of other threads or services jobs that&#8217;s why they communicate each other. For example, you can run several threads doing same job in parallel or several nodes doing the same job in parallel.</p>
</div>
<div class="paragraph">
<p>Processes and threads could communicate with each other sending messages or using shared resource(it&#8217;s called <a href="https://en.wikipedia.org/wiki/Inter-process_communication">IPC</a>) and services could use common database, queue or send direct messages.</p>
</div>
<div class="paragraph">
<p>And communication is the main problem. I&#8217;ll give some examples, but take into account that we have exactly the same problems for threads running within a process, for processes running withing some host or services running on different hosts.</p>
</div>
<div class="paragraph">
<p>So let&#8217;s say that we have 2 threads, processes or services A and B communicating each other. To complete its job A need some data produced by B.</p>
</div>
<div class="paragraph">
<p>Some examples of communication and possible problems:
1. Synchronous communication. A directly calls B, execution of A is blocked until B provided the result.
Problems. If B have to call some other dependencies
Advantages.
2. Asynchronous communications:
2.1 A calls B and doesn&#8217;t wait for a response. B knows that A requested a result of its job and it knows how to call A and pass the result.
2.2 A calls B, doesn&#8217;t wait for a response and periodically checks if response is ready.
3. A reads data from a dependency C where the result of the operation made by B is located.
4. A is split into A1 and A2. A1, B, A2</p>
</div>
<div class="paragraph">
<p>And the only difference between threads and services is speed of the communication.</p>
</div>
</div>
</div>
</div>
                    </div>
                    <div class="height-40px"></div>
                    <div>
                        <h4 class="text-center">Like it? Share it!</h4>
                        <div class="height-15px"></div>
                        <ul class="social">
                            <li class="facebook">
                                <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fkonoplev.me&t="
                                   target="_blank" title="Share on Facebook"
                                   onclick="window.open('https://www.facebook.com/sharer/sharer.php?u=' + encodeURIComponent(document.URL)); return false;">
                	<span class="ion-social-facebook">
                                </a>
                            </li>
                            <li class="twitter">
                                <a href="https://twitter.com/intent/tweet?source=http%3A%2F%2Fkonoplev.me&text=:%20http%3A%2F%2Fkonoplev.me"
                                   target="_blank" title="Tweet"
                                   onclick="window.open('https://twitter.com/intent/tweet?text=' + encodeURIComponent(document.title) + ':%20' + encodeURIComponent(document.URL)); return false;">
                	<span class="ion-social-twitter">
                                </a>
                            </li>
                            <li class="pinterest">
                                <a href="http://pinterest.com/pin/create/button/?url=http%3A%2F%2Fkonoplev.me&description="
                                   target="_blank" title="Pin it"
                                   onclick="window.open('http://pinterest.com/pin/create/button/?url=' + encodeURIComponent(document.URL) + '&description=' +  encodeURIComponent(document.title)); return false;">
                	<span class="ion-social-pinterest">
                                </a>
                            </li>
                            <li class="reddit">
                                <a href="http://www.reddit.com/submit?url=http%3A%2F%2Fkonoplev.me&title="
                                   target="_blank" title="Submit to Reddit"
                                   onclick="window.open('http://www.reddit.com/submit?url=' + encodeURIComponent(document.URL) + '&title=' +  encodeURIComponent(document.title)); return false;">
                	<span class="ion-social-reddit">
                                </a>
                            </li>
                            <li class="linkedin">
                                <a href="http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Fkonoplev.me&title=&summary=&source=http%3A%2F%2Fkonoplev.me"
                                   target="_blank" title="Share on LinkedIn"
                                   onclick="window.open('http://www.linkedin.com/shareArticle?mini=true&url=' + encodeURIComponent(document.URL) + '&title=' +  encodeURIComponent(document.title)); return false;">
                	<span class="ion-social-linkedin">
                                </a>
                            </li>
                            <li class="email-icon">
                                <a href="mailto:?subject=&body=:%20http%3A%2F%2Fkonoplev.me"
                                   target="_blank" title="Email"
                                   onclick="window.open('mailto:?subject=' + encodeURIComponent(document.title) + '&body=' +  encodeURIComponent(document.URL)); return false;">
                	<span class="ion-email">
                                </a>
                            </li>
                        </ul>

                    </div>
                    <div>
                        <div id="disqus_thread" aria-live="polite">
                            <noscript>Please enable JavaScript to view the <a
                                    href="http://disqus.com/?ref_noscript">comments powered by
                                Disqus.</a></noscript>
                        </div>
                    </div>
                </article>

            </main>
        </div>
    </div>
</div>

<script type="text/javascript">
    var disqus_config = function () {
        this.language = "en";
    };

    // var disqus_developer = 1;
    // var disqus_identifier = 'http://konoplev.me/en/programming/Do_You_Know_How_Your_Code_Works.html';
    // var disqus_url = 'http://konoplev.me/en/programming/Do_You_Know_How_Your_Code_Works.html';

    (function () {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = 'https://konoplevsblog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>

<div id="remark42"></div>

    <footer id="site-footer">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <p class="copyright">&copy; 2019 Alexander Konoplev</p>
            </div>
        </div>
    </div>
</footer>

<!-- Mobile Menu -->
<div class="overlay overlay-hugeinc">
    <button type="button" class="overlay-close"><span class="ion-ios-close-empty"></span></button>
    <nav>
        <ul>
            <li><a href="../../../index.html">About</a></li>
            <li><a href="../../../en/thoughts/index/1.html">Thoughts</a></li>
            <li><a href="../../../en/programming/index/1.html">Programming</a></li>
        </ul>
    </nav>
</div>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
        m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53463634, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true,
        webvisor:true
    });
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/53463634" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->
<script src="../../../js/script.js"></script>

</body>
</html>
